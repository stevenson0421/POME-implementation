{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "import gymnasium\n",
    "\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "calclate discounted cumulated sum for a sequence (np array or tensor) and a discount factor\n",
    "'''\n",
    "def discounted_cumulated_sum(sequence, discount_factor):\n",
    "    if isinstance(sequence, np.ndarray):\n",
    "            return scipy.signal.lfilter([1], [1, float(-discount_factor)], sequence[::-1], axis=0)[::-1]\n",
    "    elif isinstance(sequence, torch.Tensor):\n",
    "            return torch.as_tensor(np.ascontiguousarray(scipy.signal.lfilter([1], [1, float(-discount_factor)], sequence.detach().numpy()[::-1], axis=0)[::-1]), dtype=torch.float32)\n",
    "    else:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "process state from environment\n",
    "\n",
    "current implementation is add a dimension for one channel since the state is grayscale image\n",
    "'''\n",
    "def process_state(state):\n",
    "    return np.expand_dims(state, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "set random seeds for reproducibility\n",
    "'''\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Buffer\n",
    "'''\n",
    "\n",
    "class Buffer:\n",
    "    '''\n",
    "    Buffer for storing information of trajectories experienced by POME agent,\n",
    "    including state, action, qf (TD target), reward, reward-to-go (discounted cumulated reward), value (estimation), action probability in log\n",
    "    '''\n",
    "    def __init__(self, size, state_dimension, action_dimension, discount_factor=0.99):\n",
    "        self.state_buffer = np.zeros(tuple([size]+list(state_dimension)), dtype=np.float32)\n",
    "        self.action_buffer = np.zeros(tuple([size]+list(action_dimension)), dtype=np.float32)\n",
    "        self.qf_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_to_go_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.action_logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "        self.discount_factor = discount_factor\n",
    "        self.pointer = 0\n",
    "        self.start_index = 0\n",
    "        self.max_size = size\n",
    "    \n",
    "    '''\n",
    "    store informations of one state-action pair\n",
    "    '''\n",
    "    def store(self, state, action, reward, value, action_logprobability):\n",
    "        assert self.pointer < self.max_size\n",
    "\n",
    "        self.state_buffer[self.pointer] = state\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.action_logprobability_buffer[self.pointer] = action_logprobability\n",
    "\n",
    "        self.pointer += 1\n",
    "\n",
    "    '''\n",
    "    call at the end of a trajectory, process further informations\n",
    "    last_val should be 0 if the trajectory ended; otherwise value estimation for the last state\n",
    "    '''\n",
    "    def done(self, last_value=0):\n",
    "        trajectory_slice = slice(self.start_index, self.pointer)\n",
    "        reward_list = np.append(self.reward_buffer[trajectory_slice], last_value)\n",
    "        value_list = np.append(self.value_buffer[trajectory_slice], last_value)\n",
    "\n",
    "        # calculate Q_ft in paper\n",
    "        self.qf_buffer[trajectory_slice] = reward_list[:-1] + self.discount_factor * value_list[1:]\n",
    "\n",
    "        # calculate reward-to-go (discounted cumulated reward)\n",
    "        self.reward_to_go_buffer[trajectory_slice] = discounted_cumulated_sum(reward_list, self.discount_factor)[:-1]\n",
    "\n",
    "        # reset start index\n",
    "        self.start_index = self.pointer\n",
    "\n",
    "    '''\n",
    "    get all data from the buffer\n",
    "    '''\n",
    "    def get(self):\n",
    "        assert self.pointer == self.max_size\n",
    "\n",
    "        # reset buffer\n",
    "        self.pointer = 0\n",
    "        self.start_index = 0\n",
    "\n",
    "        data = dict(state=self.state_buffer,\n",
    "                    action=self.action_buffer,\n",
    "                    qf=self.qf_buffer,\n",
    "                    reward=self.reward_buffer,\n",
    "                    reward_to_go=self.reward_to_go_buffer,\n",
    "                    value=self.value_buffer,\n",
    "                    action_logprobability=self.action_logprobability_buffer)\n",
    "        return {key: torch.as_tensor(value, dtype=torch.float32) for key, value in data.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Policy Network\n",
    "\n",
    "current implementation\n",
    "input state 1x210x160\n",
    "conv2d (output dim=16, kernel_size=8, stride=4, no padding) 16x51x39\n",
    "conv2d (output dim=32, kernel_size=4, stride=2, no padding) 32x24x18\n",
    "linear (output dim=256) ## note that output of this layer is also propogated to value network as input\n",
    "linear (output dim=6) ## output one-hot action\n",
    "\n",
    "output includes latent, policy and policy distribution\n",
    "'''\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self, state_dimension, action_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=state_dimension[0],\n",
    "                                      out_channels=16,\n",
    "                                      kernel_size=8,\n",
    "                                      stride=4,\n",
    "                                      padding='valid')\n",
    "        conv1_dimension = (math.floor((state_dimension[1]-8)/4)+1, math.floor((state_dimension[2]-8)/4)+1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=16,\n",
    "                                      out_channels=32,\n",
    "                                      kernel_size=4,\n",
    "                                      stride=2,\n",
    "                                      padding='valid')\n",
    "        conv2_dimension = (math.floor((conv1_dimension[0]-4)/2)+1, math.floor((conv1_dimension[1]-4)/2)+1)\n",
    "        self.fc1 = torch.nn.Linear(in_features=conv2_dimension[0]*conv2_dimension[1]*32,\n",
    "                                      out_features=256)\n",
    "        self.fc2 = torch.nn.Linear(in_features=256,\n",
    "                                       out_features=action_dimension)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "    def forward(self, state):\n",
    "        latent = self.activation(self.conv1(state))\n",
    "        latent = self.activation(self.conv2(latent))\n",
    "        latent = self.flatten(latent)\n",
    "        latent = self.activation(self.fc1(latent))\n",
    "        policy = self.activation(self.fc2(latent))\n",
    "\n",
    "        policy_distribution = torch.distributions.Categorical(logits=policy)\n",
    "\n",
    "        return latent, policy, policy_distribution\n",
    "\n",
    "\n",
    "'''\n",
    "Value Network\n",
    "\n",
    "current implementation\n",
    "input latent from policy network 1x256\n",
    "linear (output dim=1) ## output value\n",
    "'''\n",
    "class ValueNetwork(torch.nn.Module):\n",
    "    def __init__(self, latent_dimension=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(in_features=latent_dimension,\n",
    "                                      out_features=1)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, latent):\n",
    "        value = self.activation(self.fc1(latent))\n",
    "\n",
    "        return value\n",
    "        \n",
    "'''\n",
    "Reward Network\n",
    "\n",
    "current implementation (not mentioned in paper)\n",
    "input state + action 1x211x160 (action is one-hot encoded to dimension 1x1x160)\n",
    "conv2d (output dim=16, kernel_size=8, stride=4, no padding) 16x51x39\n",
    "linear (output dim=1) ## output reward\n",
    "'''\n",
    "class RewardNetwork(torch.nn.Module):\n",
    "    def __init__(self, state_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        state_action_dimension = (state_dimension[0], state_dimension[1]+1, state_dimension[2])\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=state_action_dimension[0],\n",
    "                                      out_channels=16,\n",
    "                                      kernel_size=8,\n",
    "                                      stride=4,\n",
    "                                      padding='valid')\n",
    "        conv1_dimension = (math.floor((state_action_dimension[1]-8)/4)+1, math.floor((state_action_dimension[2]-8)/4)+1)\n",
    "        self.fc1 = torch.nn.Linear(in_features=conv1_dimension[0]*conv1_dimension[1]*16,\n",
    "                                      out_features=1)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        # concatenate state and action\n",
    "        action_one_hot = torch.nn.functional.one_hot(action.to(torch.int64), num_classes=state.shape[3])\n",
    "        action_one_hot = action_one_hot.unsqueeze(1).unsqueeze(1)\n",
    "        state_action = torch.cat((state, action_one_hot), dim=2)\n",
    "\n",
    "        reward = self.conv1(state_action)\n",
    "        reward = self.flatten(reward)\n",
    "        reward = self.fc1(reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "'''\n",
    "Reward Network\n",
    "\n",
    "current implementation (parameters not mentioned in paper)\n",
    "input state + action 1x211x160 (action is one-hot encoded to dimension 1x1x160)\n",
    "conv2d (output dim=16, kernel_size=8, stride=4, no padding) 16x51x39\n",
    "linear (output dim=210x160) output new state ## note that this is a flattened image\n",
    "'''\n",
    "class TransitionNetwork(torch.nn.Module):\n",
    "    def __init__(self, state_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        state_action_dimension = (state_dimension[0], state_dimension[1]+1, state_dimension[2])\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=state_action_dimension[0],\n",
    "                                      out_channels=16,\n",
    "                                      kernel_size=8,\n",
    "                                      stride=4,\n",
    "                                      padding='valid')\n",
    "        conv1_dimension = (math.floor((state_action_dimension[1]-8)/4)+1, math.floor((state_action_dimension[2]-8)/4)+1)\n",
    "        self.fc1 = torch.nn.Linear(in_features=conv1_dimension[0]*conv1_dimension[1]*16,\n",
    "                                      out_features=state_dimension[1]*state_dimension[2])\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        # concatenate state and action\n",
    "        action_one_hot = torch.nn.functional.one_hot(action.to(torch.int64), num_classes=state.shape[3])\n",
    "        action_one_hot = action_one_hot.unsqueeze(1).unsqueeze(1)\n",
    "        state_action = torch.cat((state, action_one_hot), dim=2)\n",
    "\n",
    "        transition = self.conv1(state_action)\n",
    "        transition = self.flatten(transition)\n",
    "        transition = self.fc1(transition)\n",
    "\n",
    "        return transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Network\n",
    "\n",
    "combine four networks\n",
    "\n",
    "the functions can be further optimized\n",
    "'''\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        assert isinstance(action_space, gymnasium.spaces.Discrete)\n",
    "\n",
    "        state_dimension = (1, state_space.shape[0], state_space.shape[1])\n",
    "        action_dimension = action_space.n\n",
    "\n",
    "        self.policy_network = PolicyNetwork(state_dimension, action_dimension)\n",
    "        self.value_network = ValueNetwork()\n",
    "        self.reward_network = RewardNetwork(state_dimension)\n",
    "        self.transition_network = TransitionNetwork(state_dimension)\n",
    "\n",
    "    '''\n",
    "    get the log probability of an action\n",
    "    '''\n",
    "    def get_action_logprobability(self, policy_distribution, action):\n",
    "        action_logprobability = policy_distribution.log_prob(action)\n",
    "        \n",
    "        return action_logprobability\n",
    "\n",
    "    '''\n",
    "    get action, log probability of the action and value estimation from a state\n",
    "    '''\n",
    "    def step(self, state):\n",
    "        with torch.no_grad():\n",
    "            latent, policy, policy_distribution = self.policy_network(state)\n",
    "            \n",
    "            action = policy_distribution.sample()\n",
    "            action_logprobability = policy_distribution.log_prob(action)\n",
    "            value = self.value_network(latent)\n",
    "        return action.numpy(), action_logprobability.numpy(), value.numpy()\n",
    "    \n",
    "    '''\n",
    "    get action only from a state\n",
    "\n",
    "    this function is not used yet, maybe for evaluation\n",
    "    '''\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            latent, policy, policy_distribution = self.policy_network(state)\n",
    "            action = policy_distribution.sample()\n",
    "        return action.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pome\n",
    "\n",
    "main implementation of the method\n",
    "\n",
    "input parameters\n",
    "\n",
    "    environment: gym environment ex. gym.make('PongNoFrameskip-v4')\n",
    "\n",
    "    networkclass: combined network\n",
    "\n",
    "    number_of_epoch: the number of epoch in training process\n",
    "\n",
    "    steps_per_trajectory: steps of a trajectory\n",
    "\n",
    "    max_steps_per_trajectory: max steps of a trajectory, basically the time limit of agent (truncated if set)\n",
    "\n",
    "    pome_learning_rate: learning rate of pome object, consist of policy, value and transition\n",
    "\n",
    "    reward_learning_rate: learning rate of reward estimation, ## note that this is not mentioned in paper\n",
    "\n",
    "    train_pome_iterations: how many iterations to update pome parameters in one update ## note that this is not mentioned in paper\n",
    "    \n",
    "    train_reward_iterations: how many iterations to update reward parameters in one update ## note that this is not mentioned in paper\n",
    "\n",
    "    discount_factor: discount factor (gamma) of computing discounted cumulated sum\n",
    "\n",
    "    alpha: coefficient used in POME TD error\n",
    "\n",
    "    clip_ratio: clip ratio for POME object before combined (l^POME in paper)\n",
    "\n",
    "    value_loss_ratio: coefficient of value loss for combined POME object\n",
    "\n",
    "    transition_loss_ratio: coefficient of transition loss for combined POME object\n",
    "\n",
    "    seed: seed for randomization\n",
    "\n",
    "'''\n",
    "def pome(environment,\n",
    "         networkclass,\n",
    "         number_of_epoch,\n",
    "         steps_per_trajectory,\n",
    "         max_steps_per_trajectory,\n",
    "         pome_learning_rate,\n",
    "         reward_learning_rate,\n",
    "         train_pome_iterations,\n",
    "         train_reward_iterations,\n",
    "         discount_factor,\n",
    "         alpha,\n",
    "         clip_ratio,\n",
    "         value_loss_ratio,\n",
    "         transition_loss_ratio,\n",
    "         seed\n",
    "         ):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    # current implementation (210, 160)\n",
    "    state_space = environment.observation_space\n",
    "    # current implementation Discrete(6)\n",
    "    action_space = environment.action_space\n",
    "\n",
    "    network = networkclass(state_space, action_space)\n",
    "\n",
    "    # state dimension is (1, 210, 160), action dimension is 0 (scalar)\n",
    "    buffer = Buffer(steps_per_trajectory, tuple([1]+list(state_space.shape)), action_space.shape, discount_factor)\n",
    "\n",
    "    '''\n",
    "    pome loss before combined\n",
    "\n",
    "    note that this should be modified to combined version\n",
    "    '''\n",
    "    def get_pome_loss(data):\n",
    "        state = data['state']\n",
    "        action = data['action']\n",
    "        qf = data['qf']\n",
    "        \n",
    "        # pi_old, note that this is fixed during one epoch update\n",
    "        action_logprobability_old = data['action_logprobability']\n",
    "\n",
    "        # get policy, action probability in log and value\n",
    "        latent, policy, policy_distribution = network.policy_network(state)\n",
    "        value = network.value_network(latent)\n",
    "        action_logprobability = policy_distribution.log_prob(action)\n",
    "\n",
    "        # calculate Q_b_t in paper\n",
    "        reward_hat = network.reward_network(state, action)\n",
    "        transition = network.transition_network(state, action)\n",
    "        transition_reshaped = transition.reshape(state.shape)\n",
    "        qb = reward_hat + discount_factor * network.value_network(transition_reshaped)\n",
    "\n",
    "        # calculate pi / pi_old\n",
    "        policy_ratio = torch.exp(action_logprobability-action_logprobability_old)\n",
    "\n",
    "        # calculate epsilon and epsilon_bar in paper\n",
    "        epsilon = torch.abs(qf - qb)\n",
    "        epsilon_median = torch.median(epsilon)\n",
    "        delta_t = torch.abs(qf - value)\n",
    "        delta_t_pome = qf + alpha * torch.clamp(epsilon-epsilon_median, -delta_t, delta_t) - value\n",
    "\n",
    "        # calculate a_t_pome in paper\n",
    "        a_t_pome = discounted_cumulated_sum(delta_t_pome, 1)\n",
    "\n",
    "        # calculate pome loss in paper\n",
    "        clip_a_t_pome = torch.clamp(policy_ratio, 1-clip_ratio, 1+clip_ratio) * a_t_pome\n",
    "        pome_loss = -(torch.min(policy_ratio * a_t_pome, clip_a_t_pome)).mean()\n",
    "\n",
    "        # return a_t_pome and value for value loss\n",
    "        other_variable = {'a_t_pome': a_t_pome, 'value': value}\n",
    "\n",
    "        return pome_loss, other_variable\n",
    "    \n",
    "    '''\n",
    "    get value loss\n",
    "    '''\n",
    "    def get_value_loss(data, other_variable):\n",
    "        reward_to_go = data['reward_to_go']\n",
    "        value = other_variable['value']\n",
    "        a_t_pome = other_variable['a_t_pome']\n",
    "\n",
    "        # calculate value loss in paper\n",
    "        value_loss = ((a_t_pome + reward_to_go - value) ** 2).mean()\n",
    "\n",
    "        return value_loss\n",
    "    \n",
    "    '''\n",
    "    get reward loss\n",
    "\n",
    "    note that this should be updated seperated with pome loss\n",
    "    '''\n",
    "    def get_reward_loss(data):\n",
    "        state = data['state']\n",
    "        action = data['action']\n",
    "        reward = data['reward']\n",
    "\n",
    "        reward_hat = network.reward_network(state, action)\n",
    "\n",
    "        # calculate reward loss in paper\n",
    "        reward_loss = (torch.sum(reward - reward_hat) ** 2)\n",
    "\n",
    "        return reward_loss\n",
    "    \n",
    "    '''\n",
    "    get transition loss\n",
    "\n",
    "    note that state of 0:-1 are used for calculation, not sure if this is correct\n",
    "    '''\n",
    "    def get_transition_loss(data):\n",
    "        state = data['state']\n",
    "        action = data['action']\n",
    "        transition = network.transition_network(state[:-1], action[:-1])\n",
    "        transition_reshaped = transition.reshape(state.shape)\n",
    "\n",
    "        # calculate transition loss in paper\n",
    "        transition_loss = (torch.norm(state[:-1]-transition, p=2) ** 2)\n",
    "\n",
    "        return transition_loss\n",
    "\n",
    "    pome_optimizer = torch.optim.Adam([{'params':network.policy_network.parameters()},\n",
    "                                       {'params':network.value_network.parameters()},\n",
    "                                       {'params':network.transition_network.parameters()},], lr=pome_learning_rate)\n",
    "    # learning rate is linearly annealed [1, 0]\n",
    "    torch.optim.lr_scheduler.LinearLR(optimizer=pome_optimizer, start_factor=1., end_factor=0.)\n",
    "\n",
    "    reward_optimizer = torch.optim.Adam(params=network.reward_network.parameters(), lr=reward_learning_rate)\n",
    "    \n",
    "    '''\n",
    "    update all network parameters in paper. first update pome, then reward\n",
    "    '''\n",
    "    def update():\n",
    "        # get all data from buffer, which should be a trajectory\n",
    "        data = buffer.get()\n",
    "\n",
    "        # update pome parameters\n",
    "        for i in range(train_pome_iterations):\n",
    "            pome_optimizer.zero_grad()\n",
    "\n",
    "            pome_loss, other_variable = get_pome_loss(data)\n",
    "            value_loss = get_value_loss(data, other_variable)\n",
    "            transition_loss = get_transition_loss(data)\n",
    "\n",
    "            total_pome_loss = pome_loss + value_loss_ratio * value_loss + transition_loss_ratio * transition_loss\n",
    "            total_pome_loss.backward()\n",
    "\n",
    "            pome_optimizer.step()\n",
    "        \n",
    "        # update reward parameters\n",
    "        for i in range(train_reward_iterations):\n",
    "            reward_optimizer.zero_grad()\n",
    "\n",
    "            reward_loss = get_reward_loss(data)\n",
    "            reward_loss.backward()\n",
    "\n",
    "            reward_optimizer.step()\n",
    "\n",
    "    # main process\n",
    "    start_time = time.time()\n",
    "    state, info = environment.reset()\n",
    "    state = process_state(state)\n",
    "    \n",
    "    trajectory_reward = 0\n",
    "    trajectory_length = 0\n",
    "    trajectory_rewards = []\n",
    "\n",
    "    for epoch in range(number_of_epoch):\n",
    "        for t in range(steps_per_trajectory):\n",
    "            # get action, action probability in log, value from a state. note that state is unsqueezed as a batch with size 1\n",
    "            action, action_logprobability, value = network.step(torch.as_tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "            # get scalar\n",
    "            action = action.item()\n",
    "            action_logprobability = action_logprobability.item()\n",
    "            value = value.item()\n",
    "            # agent interaction\n",
    "            next_state, reward, terminated, truncated, info = environment.step(action)\n",
    "            trajectory_reward += reward\n",
    "            trajectory_length += 1\n",
    "            # store all data to buffer\n",
    "            buffer.store(state, action, reward, value, action_logprobability)\n",
    "            # update state. this is important!\n",
    "            state = process_state(next_state)\n",
    "            # set timeout for agent\n",
    "            timeout = (trajectory_length == max_steps_per_trajectory)\n",
    "            done = (terminated or truncated or timeout)\n",
    "            # epoch is ended with full steps\n",
    "            epoch_ended = (t == steps_per_trajectory-1)\n",
    "            # end trajectory. note that timeout and epoch_ended are same if we set the same boundary\n",
    "            if done or epoch_ended:\n",
    "                if epoch_ended and not done:\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%trajectory_length)\n",
    "\n",
    "                if timeout or epoch_ended:\n",
    "                    _, last_value, _ = network.step(torch.as_tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                else:\n",
    "                    last_value = 0\n",
    "                buffer.done(last_value)\n",
    "                # reset the environment. this is important!\n",
    "                state = environment.reset()\n",
    "                state = process_state(next_state)\n",
    "                # record total reward and reset\n",
    "                print(f'Final step {t}: trajectory_reward: {trajectory_reward}, trajectory_length: {trajectory_length}')\n",
    "                trajectory_rewards.append(trajectory_reward)\n",
    "                trajectory_reward = 0\n",
    "                trajectory_length = 0\n",
    "        \n",
    "        update()\n",
    "    print(f'Train Time: {(time.time() - start_time):2f} seconds')\n",
    "    # average reward of last 100 trajectories in paper\n",
    "    print(f'Train Score: {np.mean(trajectory_rewards[-100:])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pome(environment=gymnasium.make('PongNoFrameskip-v4', obs_type='grayscale'),\n",
    "     networkclass=Network,\n",
    "     number_of_epoch=1,\n",
    "     steps_per_epoch=2,\n",
    "     pome_learning_rate=2.5*1e-4,\n",
    "     reward_learning_rate=0.01,\n",
    "     train_pome_iterations=2,\n",
    "     train_reward_iterations=2,\n",
    "     max_trajectory_length=2,\n",
    "     discount_factor=0.99,\n",
    "     alpha=0.1,\n",
    "     clip_ratio=0.1,\n",
    "     value_loss_ratio=1,\n",
    "     transition_loss_ratio=2,\n",
    "     seed=24)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
